{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395e1987-0e34-4650-9dd6-a92aceeb1484",
   "metadata": {},
   "source": [
    "# Tarefa 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe3099-aa58-4830-bc69-e6bf794f6d8d",
   "metadata": {},
   "source": [
    "### 1. Monte um passo a passo para o algoritmo RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0da59b-a61e-451f-a85e-8b2b60ec7e6f",
   "metadata": {},
   "source": [
    "Semelhante ao Bagging, o Random Forest segue os seguintes passos:\n",
    "\n",
    "<b> 1. Bootstrap + Feature Selection (Amostragem e Seleção de Atributos): </b>\n",
    "\n",
    "    Amostragem com reposição (Bootstrap): Semelhante ao Bagging, o primeiro passo no Random Forest é realizar a amostragem aleatória com reposição. A partir do conjunto de dados original, são criadas várias amostras bootstrap, ou seja, subconjuntos de dados de treinamento com reposição, onde alguns exemplos podem ser selecionados mais de uma vez e outros podem ser omitidos.\n",
    "\n",
    "    Seleção aleatória de atributos (Feature Selection): Diferente do Bagging, onde todas as variáveis estão disponíveis para cada divisão da árvore, no Random Forest, em cada divisão de cada árvore, apenas um subconjunto aleatório de variáveis é considerado.\n",
    "\n",
    "    Para problemas de classificação, geralmente é recomendado selecionar a raiz quadrada do número total de variáveis.\n",
    "    Para problemas de regressão, um terço das variáveis é frequentemente utilizado.\n",
    "\n",
    "<b> 2. Modelagem com Árvores de Decisão: </b>\n",
    "\n",
    "    Em cada subconjunto bootstrap gerado no passo anterior, uma árvore de decisão é treinada de forma independente, utilizando o subconjunto de variáveis aleatórias selecionado.\n",
    "    A árvore é construída recursivamente, dividindo os dados em nós com base nos atributos selecionados. O processo de construção continua até que a árvore atinja um critério de parada, como profundidade máxima ou número mínimo de amostras por nó.\n",
    "\n",
    "<b> 3. Agregação (Combinação das Previsões): </b>\n",
    "    \n",
    "    Após o treinamento das árvores, o Random Forest realiza a agregação das previsões de todas as árvores de decisão para formar uma previsão final. A forma de agregação depende do tipo de problema:\n",
    "\n",
    "    Classificação: A previsão final é obtida através de votação majoritária, ou seja, a classe mais frequentada entre as previsões das árvores é selecionada como a classe final.\n",
    "    Regressão: A previsão final é obtida através da média das previsões de todas as árvores.\n",
    "    \n",
    "    Isso resulta em uma previsão final mais robusta e confiável do que as previsões individuais de cada árvore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77563b0d-2fb8-4310-a6a4-6c2615d14359",
   "metadata": {},
   "source": [
    "### 2. Explique com suas palavras o Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65077605-e5e3-43d0-a735-86393aff6a66",
   "metadata": {},
   "source": [
    "O Random Forest é uma técnica avançada de combinação de modelos que utiliza o conceito de Bagging e o aprimora, especificamente para árvores de decisão.\n",
    "\n",
    "Assim como o Bagging, o Random Forest treina vários modelos independentes em subconjuntos de dados aleatórios chamados de amostras bootstrap. No entanto, o que diferencia o Random Forest é o uso de seleção aleatória de atributos em cada divisão da árvore de decisão, além da amostragem dos dados.\n",
    "\n",
    "Essa abordagem tem dois principais benefícios:\n",
    "\n",
    "Redução da variância: A aleatoriedade adicional na escolha de atributos em cada divisão reduz a correlação entre as árvores, o que diminui a variância do modelo e melhora sua generalização.\n",
    "Menor risco de overfitting: Como as árvores são mais independentes, o modelo tende a ter uma maior capacidade de generalização, minimizando o risco de overfitting.\n",
    "No final, o Random Forest combina as previsões de suas múltiplas árvores de decisão para fazer uma previsão final de forma mais robusta, utilizando votação para classificação e média para regressão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3321aea1-0e7f-499a-98c4-bd76af2b2360",
   "metadata": {},
   "source": [
    "### 3. Qual a diferença entre Bagging e Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feef8e3-74dd-45c1-9035-a19c32c00455",
   "metadata": {},
   "source": [
    "A principal diferença entre o Bagging e o Random Forest está na seleção aleatória de atributos. O Bagging cria múltiplos modelos independentes a partir de amostras aleatórias do conjunto de dados original, mas utiliza todas as variáveis disponíveis para cada modelo. Já o Random Forest adiciona um nível extra de aleatoriedade ao selecionar um subconjunto aleatório de atributos para cada divisão da árvore, o que torna as árvores mais independentes entre si.\n",
    "\n",
    "| Característica                     | Bagging                                          | Random Forest                                                                 |\n",
    "|-------------------------------------|-------------------------------------------------|-------------------------------------------------------------------------------|\n",
    "| **Base Learners**                   | Qualquer modelo (geralmente árvores de decisão) | Árvores de Decisão                                                            |\n",
    "| **Amostragem (Bootstrap)**          | Sim                                             | Sim                                                                           |\n",
    "| **Seleção Aleatória de Atributos**  | Não                                             | Sim (apenas um subconjunto de atributos é considerado em cada divisão)        |\n",
    "| **Agregação de Resultados**         | Votação (classificação) / Média (regressão)     | Votação (classificação) / Média (regressão)                                   |\n",
    "| **Vantagem Principal**              | Reduz a variância entre os modelos              | Reduz ainda mais a variância e melhora a generalização ao reduzir a correlação entre as árvores |\n",
    "\n",
    "Conclusão:\n",
    "O Random Forest funciona melhor que o Bagging porque as árvores de decisão são mais independentes entre si, graças à seleção aleatória de atributos em cada divisão. Isso leva a uma menor correlação entre as árvores, melhorando a robustez do modelo e sua capacidade de generalização. Como resultado, o Random Forest é mais eficiente e preciso, principalmente quando comparado ao Bagging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d82d9b-40ec-46e6-ab7d-032745ae44d7",
   "metadata": {},
   "source": [
    "### 4. (Opcional) Implementar em python o código do Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a599e4d1-4485-4bef-9ce7-b2ef2ca10b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bagging Model: 0.9450\n",
      "Accuracy of Aggregated Predictions (Manual): 0.9550\n",
      "Features selecionadas: [0 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Gerar um conjunto de dados de classificação fictício\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
    "\n",
    "# Seleção de features: Escolher as K melhores características\n",
    "k = 5  # Número de melhores features a serem selecionadas\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_new = selector.fit_transform(X, y)  # Aplicar a seleção de features\n",
    "\n",
    "# Dividir os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Etapa 1 - Bootstrap: Gerando subconjuntos de dados com reposição\n",
    "n_estimators = 10  # Número de modelos na técnica de Bagging\n",
    "\n",
    "# Criar os conjuntos de treinamento com reposição\n",
    "bootstrap_samples = [np.random.choice(X_train.shape[0], size=X_train.shape[0], replace=True) for _ in range(n_estimators)]\n",
    "\n",
    "# Etapa 2 - Modelagem: Treinando múltiplos modelos com os subconjuntos criados\n",
    "models = []\n",
    "for i in range(n_estimators):\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    model.fit(X_train[bootstrap_samples[i]], y_train[bootstrap_samples[i]])\n",
    "    models.append(model)\n",
    "\n",
    "# Etapa 3 - Agregação: Fazendo previsões com os modelos e agregando os resultados\n",
    "# Aqui usamos BaggingClassifier para simplificar o processo de agregação.\n",
    "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=n_estimators, random_state=42)\n",
    "\n",
    "# Treinar o modelo de Bagging\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = bagging.predict(X_test)\n",
    "\n",
    "# Avaliar a acurácia do modelo de Bagging\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Bagging Model: {accuracy:.4f}\")\n",
    "\n",
    "# Comparação com agregação manual das previsões individuais\n",
    "individual_predictions = [model.predict(X_test) for model in models]\n",
    "aggregated_predictions = np.array(individual_predictions).T\n",
    "aggregated_predictions = [np.bincount(pred).argmax() for pred in aggregated_predictions]\n",
    "\n",
    "# Avaliação da acurácia da agregação manual\n",
    "aggregated_accuracy = accuracy_score(y_test, aggregated_predictions)\n",
    "print(f\"Accuracy of Aggregated Predictions (Manual): {aggregated_accuracy:.4f}\")\n",
    "\n",
    "# Mostrar as features mais importantes selecionadas\n",
    "selected_features = selector.get_support(indices=True)\n",
    "print(f\"Features selecionadas: {selected_features}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
