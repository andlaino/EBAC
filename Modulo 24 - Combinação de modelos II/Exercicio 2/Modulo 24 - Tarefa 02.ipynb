{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a416507-e679-47e1-a203-a36a58ebc8be",
   "metadata": {},
   "source": [
    "<h2>Tarefa 02</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b078ecde-53f0-4c94-82ce-be8e37dbb4c7",
   "metadata": {},
   "source": [
    "**1. Cite 5 diferenças entre o AdaBoost e o GBM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac638477-7d09-4beb-af01-56bde1fce0e3",
   "metadata": {},
   "source": [
    "| Diferenças  | AdaBoost | GBM |\n",
    "|------------|----------|-----|\n",
    "| 1 | Utiliza um conjunto de classificadores fracos (Stumps), também conhecidos como weak/base learners. | Utiliza árvores mais complexas do que o AdaBoost. |\n",
    "| 2 | As árvores de decisão são extremamente simples com apenas 1 de profundidade e 2 folhas. | As árvores complexas podem ser podadas ou não. |\n",
    "| 3 | O primeiro passo do modelo é a seleção de um Stump com o melhor desempenho. | O primeiro passo do modelo é o cálculo da média do Y (resposta/target). |\n",
    "| 4 | Cada resposta tem um peso diferente para a agregação final da predição. | A predição das respostas das árvores é calculada através de um multiplicador em comum chamado learning_rate (eta). |\n",
    "| 5 | A predição final é definida com uma votação majoritária ponderada das respostas de acordo com a performance de cada Stump. | A predição é baseada no ajuste do modelo através da redução dos erros residuais. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173be6f-5ca5-4301-8627-5a4a14448254",
   "metadata": {},
   "source": [
    "**2. Acesse o link Scikit-learn – GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dbcd5b-e3b4-456c-80d7-16867f9f942a",
   "metadata": {},
   "source": [
    "Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df75b177-e440-4507-8e77-69587908656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc6bedfd-de07-46ce-8702-e2a13694ca74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, \n",
    "                                 learning_rate=1.0, \n",
    "                                 max_depth=1, \n",
    "                                 random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d11f83a-9ac7-4722-8ce4-b3802bbdd884",
   "metadata": {},
   "source": [
    "Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2da1725-ca5e-426e-a371-980261278ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b2488e2-e917-4991-b5ec-3071a1d6fb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "est = GradientBoostingRegressor(n_estimators=100, \n",
    "                                learning_rate=0.1, \n",
    "                                max_depth=1, \n",
    "                                random_state=0, \n",
    "                                loss='squared_error').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca88e1-2a01-470a-8226-67ca8f9f673f",
   "metadata": {},
   "source": [
    "**3. Cite 5 hiperparâmetros importantes no GBM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e15657-a195-4436-b09d-4572b911c024",
   "metadata": {},
   "source": [
    "**_1. n_estimators:_** Este importante parâmetro controla o número de árvores no modelo. Um valor maior pode aumentar tanto o tempo de treinamento quanto a capacidade de aprendizado do modelo.\n",
    "\n",
    "**_2. learning_rate:_** É um importante hiperparâmetro no intervalo de 0.0 a 1.0 que controla a taxa de aprendizado do modelo. Interage fortemente com o número de estimadores (n_estimators). O valor ideal pode variar dependendo do problema e dos dados.\n",
    "\n",
    "**_3. max_depth:_** Controla a profundidade máxima de cada árvore no modelo, especificando o número máximo de nós. Uma profundidade maior permite que as árvores sejam mais complexas e tenham maior capacidade de aprendizado, mas também aumenta o risco de overfitting.\n",
    "\n",
    "**_4. max_features:_** Controla o número de variáveis consideradas ao fazer uma divisão em cada nó das árvores. Quanto menor o valor, menor é o risco de overfitting.\n",
    "\n",
    "**_5. warm_start:_** Permite adicionar mais estimadores a um modelo já ajustado. Pode ser útil quando é necessário ajustar o modelo com mais árvores sem perder o progresso anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f76f5-debd-493b-b325-d7729df657c5",
   "metadata": {},
   "source": [
    "**4. (Opcional) Utilize o GridSearch para encontrar os melhores hiperparâmetros para o conjunto de dados do exemplo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cdf6dd4-1ddf-44f8-a209-e65dc79929b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770528c-b01f-4846-9844-421575f7c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimators     = [10, 100, 1000, 10000]\n",
    "learning_rates = [0.01, 0.03, 0.06, 0.1]\n",
    "max_depths     = [1, 3, 6, 9]\n",
    "\n",
    "grid_search = []\n",
    "\n",
    "for n in estimators:\n",
    "    for rate in learning_rates:\n",
    "        for depth in max_depths:\n",
    "            est = GradientBoostingRegressor(n_estimators=n, \n",
    "                                            learning_rate=rate, \n",
    "                                            max_depth=depth, \n",
    "                                            random_state=0, \n",
    "                                            loss='squared_error').fit(X_train, y_train)\n",
    "            grid_search.append([n, rate, depth, mean_squared_error(y_test, est.predict(X_test))])\n",
    "            \n",
    "(pd.DataFrame(data=grid_search, \n",
    "              columns=['n_estimators', 'learning_rate', 'max_depth', 'mean_squared_error'])\n",
    "   .sort_values(by='mean_squared_error', \n",
    "                ascending=True, \n",
    "                ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37934e7b-dc2b-4818-bde6-2629610275a6",
   "metadata": {},
   "source": [
    "**5. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b8361-7f0d-4a5a-8fc5-a857b09e6b4c",
   "metadata": {},
   "source": [
    "Pensando no nome dado ao Stochastic GBM e com referência à teoria probabilística, podemos descrevê-lo como um algoritmo que incorpora variáveis aleatórias. O Stochastic GBM é uma combinação dos métodos de Gradient Boosting e Bootstrap Aggregating, sendo considerado um híbrido das técnicas Bagging e Boosting. Em cada iteração, o classificador base é treinado em um subconjunto aleatório e não repetitivo dos dados de treinamento, utilizando em média metade da amostra. Essa aleatoriedade na amostragem do conjunto de dados em cada iteração de treino melhora significativamente a precisão do Gradient Boosting e torna o modelo mais robusto em comparação com o GBM tradicional. Isso ocorre porque a aleatoriedade ajuda a evitar o overfitting e promove uma melhor generalização do modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
